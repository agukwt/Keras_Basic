Cul.02のテーマ
「MNISTでのDNNを構築し、中断から再実行可能なプログラムを作成する」

ステップ1：
    データ: MNIST
    モデル構築：
        構造：ファンクションAPI型
        層：DNN(入力728次元、バッチノルム有、多重層(5＋))
        活性化関数: 任意(leru推奨)
    モデル実行：
        ロス関数：categorical_crossentropy
        最適化関数：任意(Adam(lr=0.01, beta_1=0.9, beta_2=0.999)推奨)
        コールバック：TensorBoard, ModelCheckpointを作成する
        その他：epochsを20とする。

    モデル評価：
         loss および accuracy を表示


    注意事項：
        ・ファンクションAPI型とする意図は、各クラスとメソッドを再利用するためである。
        　すなわち、最終的には凝集度(1つのメソッドに１つの役割)を考慮すること。

ステップ2：
    ステップ1の学習データ(コールバック)を用い、再学習せよ。

ステップ3：
    ステップ2におけるモデル構築の層についてCNNを用い、学習せよ。

ステップ4：
    ステップ1~3までの実行コードをソフトウェア観点で統合・整理せよ。

    【観点】
    ・DNN系モデリング利用とCNN系モデリング利用がある
    ・学習と再学習の実行形態が必要
    ・データセットはMNIST前提とする
    ・前処理ではDNNとCNNで処理が異なる
    ・モデルはDNN種(デフォルトと自作等)とCNN種(LeNetやその他)が定義できるようにする
    ・エラーハンドリング
    ・ログ(コールバックではない)出し
    ・pydocおよびコメントによる、コード補足
    ・1バッチが多量だった場合のfit_generator利用についての考察

    【実施事項】
    TODO 001:   load_data() にデータロード機能のみを付与
    TODO 002:   001の変更に対応し、preprocess機能をCNNとDNNで分離
    TODO 003:   001-2の変更に対応し、CNNまたはDNNの学習を設定
    TODO 004:   CNNおよびDNNの学習再開を設定
    TODO 005:   新たなDNN model(隠れ層内容(数やニューロンリスト等)が変数のタイプ)を設定、DNNModelerを整理
    TODO 006:   新たなCNN modelを設定、DNNModelerを整理

    □:   TensorBoard, ModelCheckpoint, EarlyStoppingを関数としての分離
    □:   model path でのexist証明
    □:   fit_generator






